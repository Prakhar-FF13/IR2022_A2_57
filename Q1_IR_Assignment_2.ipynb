{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13d095b",
   "metadata": {},
   "source": [
    "# IR Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407d7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ba7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89636b",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "\n",
    "## Jaccard Coefficient\n",
    "<br>\n",
    "The goal is to find the Jaccard coefficient between a given query and the document. The formula used is\n",
    "mentioned below as:\n",
    "<br>\n",
    "Jaccard Coefficient = Intersection of (doc,query) / Union of (doc,query)\n",
    "<br>\n",
    "The high the value of the Jaccard coefficient, the more the document is relevant for the query.\n",
    "\n",
    "1. Use the same data given in assignment 1 and carry out the same preprocessing steps as mentioned\n",
    "before.\n",
    "2. To calculate this make set of the document token and query token and perform intersection and union\n",
    "between the query and each document.\n",
    "3. Report the top 5 relevant documents based on the value of the Jaccard coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cb1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d29924",
   "metadata": {},
   "source": [
    "1. Preprocessing steps as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574159a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcessing Steps\n",
    "\n",
    "def preProcess(file):\n",
    "    # Convert file to lowercase.\n",
    "    file = file.lower()\n",
    "    \n",
    "    # Word Tokenization\n",
    "    file = word_tokenize(file)\n",
    "    \n",
    "    # Removing stopwords\n",
    "    token_list = []\n",
    "    for word in file:\n",
    "        if word not in stopWords:\n",
    "            token_list.append(word)\n",
    "    \n",
    "    # Remove punctuations.\n",
    "    tokens = [word.translate(str.maketrans('', '', '\\t')) for word in token_list]\n",
    "\n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\")\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in tokens]\n",
    "    tokens = [str for str in stripped_words if str]\n",
    "    \n",
    "    #Removing blank tokens\n",
    "    while(\"\" in tokens) :\n",
    "        tokens.remove(\"\")\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0477f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\jsk\\Desktop\\IIITD\\SEMESTER 2\\IR\\Assignment\\Files\"\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfea080e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1st_aid.txt',\n",
       " 'a-team',\n",
       " 'abbott.txt',\n",
       " 'aboutada.txt',\n",
       " 'acetab1.txt',\n",
       " 'aclamt.txt',\n",
       " 'acne1.txt',\n",
       " 'acronym.lis',\n",
       " 'acronym.txt',\n",
       " 'acronyms.txt',\n",
       " 'adameve.hum',\n",
       " 'adcopy.hum',\n",
       " 'addrmeri.txt',\n",
       " 'admin.txt',\n",
       " 'adrian_e.faq',\n",
       " 'ads.txt',\n",
       " 'adt_miam.txt',\n",
       " 'advrtize.txt',\n",
       " 'aeonint.txt',\n",
       " 'age.txt',\n",
       " 'aggie.txt',\n",
       " 'aids.txt',\n",
       " 'airlines',\n",
       " 'alabama.txt',\n",
       " 'alcatax.txt',\n",
       " 'alcohol.hum',\n",
       " 'alflog.txt',\n",
       " 'allfam.epi',\n",
       " 'allusion',\n",
       " 'all_grai',\n",
       " 'amazing.epi',\n",
       " 'ambrose.bie',\n",
       " 'amchap2.txt',\n",
       " 'analogy.hum',\n",
       " 'aniherb.txt',\n",
       " 'anime.cli',\n",
       " 'anime.lif',\n",
       " 'anim_lif.txt',\n",
       " 'annoy.fascist',\n",
       " 'anorexia.txt',\n",
       " 'answers',\n",
       " 'anthropo.stu',\n",
       " 'antibiot.txt',\n",
       " 'antimead.bev',\n",
       " 'aphrodis.txt',\n",
       " 'appbred.brd',\n",
       " 'appetiz.rcp',\n",
       " 'applepie.des',\n",
       " 'apsaucke.des',\n",
       " 'apsnet.txt',\n",
       " 'arab.dic',\n",
       " 'arcadian.txt',\n",
       " 'argotdic.txt',\n",
       " 'arnold.txt',\n",
       " 'art-fart.hum',\n",
       " 'arthriti.txt',\n",
       " 'ateam.epi',\n",
       " 'atherosc.txt',\n",
       " 'atombomb.hum',\n",
       " 'att.txt',\n",
       " 'aussie.lng',\n",
       " 'avengers.lis',\n",
       " 'awespinh.sal',\n",
       " 'ayurved.txt',\n",
       " 'a_fish_c.apo',\n",
       " 'a_tv_t-p.com',\n",
       " 'b-2.jok',\n",
       " 'b12.txt',\n",
       " 'back1.txt',\n",
       " 'bad',\n",
       " 'bad-d',\n",
       " 'bad.jok',\n",
       " 'badday.hum',\n",
       " 'bagelope.txt',\n",
       " 'bakebred.txt',\n",
       " 'baklava.des',\n",
       " 'banana01.brd',\n",
       " 'banana02.brd',\n",
       " 'banana03.brd',\n",
       " 'banana04.brd',\n",
       " 'banana05.brd',\n",
       " 'bank.rob',\n",
       " 'barney.cn1',\n",
       " 'barney.txt',\n",
       " 'basehead.txt',\n",
       " 'batrbred.txt',\n",
       " 'bb',\n",
       " 'bbc_vide.cat',\n",
       " 'bbh_intv.txt',\n",
       " 'bbq.txt',\n",
       " 'beapimp.hum',\n",
       " 'beauty.tm',\n",
       " 'beave.hum',\n",
       " 'beer-g',\n",
       " 'beer-gui',\n",
       " 'beer.gam',\n",
       " 'beer.hum',\n",
       " 'beer.txt',\n",
       " 'beerdiag.txt',\n",
       " 'beergame.hum',\n",
       " 'beergame.txt',\n",
       " 'beerjesus.hum',\n",
       " 'beershrm.fis',\n",
       " 'beershrp.fis',\n",
       " 'beerwarn.txt',\n",
       " 'beesherb.txt',\n",
       " 'beginn.ers',\n",
       " 'berryeto.bev',\n",
       " 'bhang.fun',\n",
       " 'bhb.ill',\n",
       " 'bible.txt',\n",
       " 'bigpic1.hum',\n",
       " 'billcat.hum',\n",
       " 'bimg.prn',\n",
       " 'bingbong.hum',\n",
       " 'bitchcar.hum',\n",
       " 'bitnet.txt',\n",
       " 'blackadd',\n",
       " 'blackapp.hum',\n",
       " 'blackhol.hum',\n",
       " 'blake7.lis',\n",
       " 'blaster.hum',\n",
       " 'bless.bc',\n",
       " 'blkbean.txt',\n",
       " 'blkbnsrc.vgn',\n",
       " 'blood.txt',\n",
       " 'blooprs1.asc',\n",
       " 'bmdn01.txt',\n",
       " 'bnbeg2.4.txt',\n",
       " 'bnbguide.txt',\n",
       " 'bnb_quot.txt',\n",
       " 'boarchil.txt',\n",
       " 'boatmemo.jok',\n",
       " 'boe.hum',\n",
       " 'bond-2.txt',\n",
       " 'boneles2.txt',\n",
       " 'booknuti.txt',\n",
       " 'booze.fun',\n",
       " 'booze1.fun',\n",
       " 'booze2.fun',\n",
       " 'bored.txt',\n",
       " 'boston.geog',\n",
       " 'bozo_tv.leg',\n",
       " 'brainect.hum',\n",
       " 'brdpudd.des',\n",
       " 'bread.rcp',\n",
       " 'bread.rec',\n",
       " 'bread.txt',\n",
       " 'breadpud.des',\n",
       " 'bredcake.des',\n",
       " 'brewing',\n",
       " 'browneco.hum',\n",
       " 'brownie.rec',\n",
       " 'brush1.txt',\n",
       " 'btaco.txt',\n",
       " 'btcisfre.hum',\n",
       " 'btscke01.des',\n",
       " 'btscke02.des',\n",
       " 'btscke03.des',\n",
       " 'btscke04.des',\n",
       " 'btscke05.des',\n",
       " 'buffwing.pol',\n",
       " 'bugbreak.hum',\n",
       " 'bugs.txt',\n",
       " 'buldrwho.txt',\n",
       " 'bunacald.fis',\n",
       " 'burrito.mea',\n",
       " 'butcher.txt',\n",
       " 'butstcod.fis',\n",
       " 'butwrong.hum',\n",
       " 'buzzword.hum',\n",
       " 'bw-phwan.hat',\n",
       " 'bw-summe.hat',\n",
       " 'bw.txt',\n",
       " 'byfb.txt',\n",
       " 'c0dez.txt',\n",
       " 'cabbage.txt',\n",
       " 'caesardr.sal',\n",
       " 'cake.rec',\n",
       " 'calamus.hrb',\n",
       " 'calculus.txt',\n",
       " 'calif.hum',\n",
       " 'calvin.txt',\n",
       " 'cancer.rat',\n",
       " 'candy.txt',\n",
       " 'candybar.fun',\n",
       " 'capital.txt',\n",
       " 'caramels.des',\n",
       " 'carowner.txt',\n",
       " 'cars.txt',\n",
       " 'cartoon.law',\n",
       " 'cartoon.laws',\n",
       " 'cartoon_.txt',\n",
       " 'cartoon_laws.txt',\n",
       " 'cartwb.son',\n",
       " 'cast.lis',\n",
       " 'catballs.hum',\n",
       " 'catin.hat',\n",
       " 'catranch.hum',\n",
       " 'catstory.txt',\n",
       " 'cbmatic.hum',\n",
       " 'cereal.txt',\n",
       " 'cform2.txt',\n",
       " 'cgs_lst.txt',\n",
       " 'chainltr.txt',\n",
       " 'change.hum',\n",
       " 'charity.hum',\n",
       " 'cheapfar.hum',\n",
       " 'cheapin.la',\n",
       " 'chickenheadbbs.txt',\n",
       " 'chickens.jok',\n",
       " 'chickens.txt',\n",
       " 'childhoo.jok',\n",
       " 'childrenbooks.txt',\n",
       " 'chili.txt',\n",
       " 'chinese.txt',\n",
       " 'chinesec.hum',\n",
       " 'choco-ch.ips',\n",
       " 'christop.int',\n",
       " 'chung.iv',\n",
       " 'chunnel.txt',\n",
       " 'church.sto',\n",
       " 'clancy.txt',\n",
       " 'classicm.hum',\n",
       " 'climbing.let',\n",
       " 'cmu.share',\n",
       " 'co-car.jok',\n",
       " 'cockney.alp',\n",
       " 'coffee.faq',\n",
       " 'coffee.txt',\n",
       " 'coffeebeerwomen.txt',\n",
       " 'cogdis.txt',\n",
       " 'coke.fun',\n",
       " 'coke.txt',\n",
       " 'coke1',\n",
       " 'cokeform.txt',\n",
       " 'coke_fan.naz',\n",
       " 'coladrik.fun',\n",
       " 'coladrik.txt',\n",
       " 'cold.fus',\n",
       " 'coldfake.hum',\n",
       " 'collected_quotes.txt',\n",
       " 'college.hum',\n",
       " 'college.sla',\n",
       " 'college.txt',\n",
       " 'comic_st.gui',\n",
       " 'commutin.jok',\n",
       " 'commword.hum',\n",
       " 'computer.txt',\n",
       " 'comrevi1.hum',\n",
       " 'conan.txt',\n",
       " 'confucius_say.txt',\n",
       " 'consp.txt',\n",
       " 'contract.moo',\n",
       " 'cookberk',\n",
       " 'cookbkly.how',\n",
       " 'cookie.1',\n",
       " 'cooking.fun',\n",
       " 'cooking.jok',\n",
       " 'coollngo2.txt',\n",
       " 'cooplaws',\n",
       " 'cops.txt',\n",
       " 'corporat.txt',\n",
       " 'court.quips',\n",
       " 'cowexplo.hum',\n",
       " 'coyote.txt',\n",
       " 'crazy.txt',\n",
       " 'critic.txt',\n",
       " 'crzycred.lst',\n",
       " 'cuchy.hum',\n",
       " 'cucumber.jok',\n",
       " 'cucumber.txt',\n",
       " 'cuisine.txt',\n",
       " 'cultmov.faq',\n",
       " 'curiousgeorgie.txt',\n",
       " 'curry.hrb',\n",
       " 'curry.txt',\n",
       " 'curse.txt',\n",
       " 'cybrtrsh.txt',\n",
       " 'd-ned.hum',\n",
       " 'dalive',\n",
       " 'damiana.hrb',\n",
       " 'dandwine.bev',\n",
       " 'dark.suc',\n",
       " 'dead-r',\n",
       " 'dead2.txt',\n",
       " 'dead3.txt',\n",
       " 'dead4.txt',\n",
       " 'dead5.txt',\n",
       " 'deadlysins.txt',\n",
       " 'deathhem.txt',\n",
       " 'deep.txt',\n",
       " 'defectiv.hum',\n",
       " 'desk.txt',\n",
       " 'deterior.hum',\n",
       " 'devils.jok',\n",
       " 'diesmurf.txt',\n",
       " 'diet.txt',\n",
       " 'dieter.txt',\n",
       " 'dingding.hum',\n",
       " 'dining.out',\n",
       " 'dirtword.txt',\n",
       " 'disaster.hum',\n",
       " 'disclmr.txt',\n",
       " 'disclym.txt',\n",
       " 'doc-says.txt',\n",
       " 'docdict.txt',\n",
       " 'docspeak.txt',\n",
       " 'doggun.sto',\n",
       " 'donut.txt',\n",
       " 'dover.poem',\n",
       " 'draxamus.txt',\n",
       " 'drinker.txt',\n",
       " 'drinking.tro',\n",
       " 'drinkrul.jok',\n",
       " 'drinks.gui',\n",
       " 'drinks.txt',\n",
       " 'drive.txt',\n",
       " 'dromes.txt',\n",
       " 'druggame.hum',\n",
       " 'drugshum.hum',\n",
       " 'drunk.txt',\n",
       " 'dthought.txt',\n",
       " 'dubltalk.jok',\n",
       " 'dym',\n",
       " 'eandb.drx',\n",
       " 'earp',\n",
       " 'eatme.txt',\n",
       " 'econridl.fun',\n",
       " 'egg-bred.txt',\n",
       " 'egglentl.vgn',\n",
       " 'eggroll1.mea',\n",
       " 'electric.txt',\n",
       " 'element.jok',\n",
       " 'elephant.fun',\n",
       " 'elevator.fun',\n",
       " 'empeval.txt',\n",
       " 'engineer.hum',\n",
       " 'english',\n",
       " 'english.txt',\n",
       " 'engmuffn.txt',\n",
       " 'engrhyme.txt',\n",
       " 'enlightenment.txt',\n",
       " 'enquire.hum',\n",
       " 'epikarat.txt',\n",
       " 'epiquest.txt',\n",
       " 'episimp2.txt',\n",
       " 'epitaph',\n",
       " 'epi_.txt',\n",
       " 'epi_bnb.txt',\n",
       " 'epi_merm.txt',\n",
       " 'epi_rns.txt',\n",
       " 'epi_tton.txt',\n",
       " 'eskimo.nel',\n",
       " 'exam.50',\n",
       " 'excuse.txt',\n",
       " 'excuse30.txt',\n",
       " 'excuses.txt',\n",
       " 'exidy.txt',\n",
       " 'exylic.txt',\n",
       " 'facedeth.txt',\n",
       " 'failure.txt',\n",
       " 'fajitas.rcp',\n",
       " 'farsi.phrase',\n",
       " 'farsi.txt',\n",
       " 'fartinfo.txt',\n",
       " 'fartting.txt',\n",
       " 'fascist.txt',\n",
       " 'fbipizza.txt',\n",
       " 'fearcola.hum',\n",
       " 'fed.txt',\n",
       " 'fegg!int.txt',\n",
       " 'feggaqui.txt',\n",
       " 'feggmagi.txt',\n",
       " 'feista01.dip',\n",
       " 'female.jok',\n",
       " 'fiber.txt',\n",
       " 'figure_1.txt',\n",
       " 'filmgoof.txt',\n",
       " 'films_gl.txt',\n",
       " 'final-ex.txt',\n",
       " 'finalexm.hum',\n",
       " 'firecamp.txt',\n",
       " 'fireplacein.txt',\n",
       " 'firstaid.inf',\n",
       " 'firstaid.txt',\n",
       " 'fish.rec',\n",
       " 'flattax.hum',\n",
       " 'flowchrt',\n",
       " 'flowchrt.txt',\n",
       " 'flux_fix.txt',\n",
       " 'focaccia.brd',\n",
       " 'food',\n",
       " 'foodtips',\n",
       " 'footfun.hum',\n",
       " 'forsooth.hum',\n",
       " 'free-cof.fee',\n",
       " 'freshman.hum',\n",
       " 'freudonseuss.txt',\n",
       " 'frogeye1.sal',\n",
       " 'from.hum',\n",
       " 'fuck!.txt',\n",
       " 'fuckyou2.txt',\n",
       " 'fudge.txt',\n",
       " 'fusion.gal',\n",
       " 'fusion.sup',\n",
       " 'fwksfun.hum',\n",
       " 'f_tang.txt',\n",
       " 'gack!.txt',\n",
       " 'gaiahuma',\n",
       " 'gameshow.txt',\n",
       " 'ganamembers.txt',\n",
       " 'garlpast.vgn',\n",
       " 'gas.txt',\n",
       " 'gd_alf.txt',\n",
       " 'gd_drwho.txt',\n",
       " 'gd_flybd.txt',\n",
       " 'gd_frasr.txt',\n",
       " 'gd_gal.txt',\n",
       " 'gd_guide.txt',\n",
       " 'gd_hhead.txt',\n",
       " 'gd_liqtv.txt',\n",
       " 'gd_maxhd.txt',\n",
       " 'gd_ol.txt',\n",
       " 'gd_ql.txt',\n",
       " 'gd_sgrnd.txt',\n",
       " 'gd_tznew.txt',\n",
       " 'german.aut',\n",
       " 'get.drunk.cheap',\n",
       " 'ghostfun.hum',\n",
       " 'ghostsch.hum',\n",
       " 'gingbeer.txt',\n",
       " 'girlspeak.txt',\n",
       " 'godmonth.txt',\n",
       " 'goforth.hum',\n",
       " 'gohome.hum',\n",
       " 'goldwatr.txt',\n",
       " 'golnar.txt',\n",
       " 'good.txt',\n",
       " 'gotukola.hrb',\n",
       " 'gown.txt',\n",
       " 'grail.txt',\n",
       " 'grammar.jok',\n",
       " 'greenchi.txt',\n",
       " 'grommet.hum',\n",
       " 'grospoem.txt',\n",
       " 'growth.txt',\n",
       " 'gumbo.txt',\n",
       " 'hack',\n",
       " 'hack7.txt',\n",
       " 'hackingcracking.txt',\n",
       " 'hackmorality.txt',\n",
       " 'hacktest.txt',\n",
       " 'hamburge.nam',\n",
       " 'hammock.hum',\n",
       " 'hangover.txt',\n",
       " 'happyhack.txt',\n",
       " 'harmful.hum',\n",
       " 'hate.hum',\n",
       " 'hbo_spec.rev',\n",
       " 'headlnrs',\n",
       " 'hecomes.jok',\n",
       " 'hedgehog.txt',\n",
       " 'height.txt',\n",
       " 'hell.jok',\n",
       " 'hell.txt',\n",
       " 'herb!.hum',\n",
       " 'hermsys.txt',\n",
       " 'heroic.txt',\n",
       " 'hi.tec',\n",
       " 'hierarch.txt',\n",
       " 'highland.epi',\n",
       " 'hilbilly.wri',\n",
       " 'history2.oop',\n",
       " 'hitchcoc.app',\n",
       " 'hitchcok.txt',\n",
       " 'hitler.59',\n",
       " 'hitler.txt',\n",
       " 'hitlerap.txt',\n",
       " 'homebrew.txt',\n",
       " 'homermmm.txt',\n",
       " 'hoonsrc.txt',\n",
       " 'hoosier.txt',\n",
       " 'hop.faq',\n",
       " 'horflick.txt',\n",
       " 'horoscop.jok',\n",
       " 'horoscop.txt',\n",
       " 'horoscope.txt',\n",
       " 'hotel.txt',\n",
       " 'hotnnot.hum',\n",
       " 'hotpeper.txt',\n",
       " 'how.bugs.breakd',\n",
       " 'how2bgod.txt',\n",
       " 'how2dotv.txt',\n",
       " 'howlong.hum',\n",
       " 'how_to_i.pro',\n",
       " 'hstlrtxt.txt',\n",
       " 'htswfren.txt',\n",
       " 'hum2',\n",
       " 'humatra.txt',\n",
       " 'humatran.jok',\n",
       " 'humor9.txt',\n",
       " 'humpty.dumpty',\n",
       " 'iced.tea',\n",
       " 'icm.hum',\n",
       " 'idaho.txt',\n",
       " 'idr2.txt',\n",
       " 'imbecile.txt',\n",
       " 'imprrisk.hum',\n",
       " 'impurmat.hum',\n",
       " 'incarhel.hum',\n",
       " 'indgrdn.txt',\n",
       " 'initials.rid',\n",
       " 'inlaws1.txt',\n",
       " 'inquirer.txt',\n",
       " 'ins1',\n",
       " 'insanity.hum',\n",
       " 'insect1.txt',\n",
       " 'insult',\n",
       " 'insult.lst',\n",
       " 'insults1.txt',\n",
       " 'insuranc.sty',\n",
       " 'insure.hum',\n",
       " 'interv.hum',\n",
       " 'investi.hum',\n",
       " 'iqtest',\n",
       " 'iremember',\n",
       " 'IR_Assignment2.ipynb',\n",
       " 'is_story.txt',\n",
       " 'italoink.txt',\n",
       " 'ivan.hum',\n",
       " 'jac&tuu.hum',\n",
       " 'jalapast.dip',\n",
       " 'jambalay.pol',\n",
       " 'japantv.txt',\n",
       " 'japice.bev',\n",
       " 'japrap.hum',\n",
       " 'jargon.phd',\n",
       " 'jason.fun',\n",
       " 'jawgumbo.fis',\n",
       " 'jawsalad.fis',\n",
       " 'jayjay.txt',\n",
       " 'jc-elvis.inf',\n",
       " 'jeffie.heh',\n",
       " 'jerky.rcp',\n",
       " 'jimhood.txt',\n",
       " 'johann',\n",
       " 'jokeju07.txt',\n",
       " 'jokes',\n",
       " 'jokes.txt',\n",
       " 'jokes1.txt',\n",
       " 'jon.txt',\n",
       " 'jrrt.riddle',\n",
       " 'jungjuic.bev',\n",
       " 'just2',\n",
       " 'justify',\n",
       " 'kaboom.hum',\n",
       " 'kanalx.txt',\n",
       " 'kashrut.txt',\n",
       " 'kid2',\n",
       " 'kid_diet.txt',\n",
       " 'killer.hum',\n",
       " 'killself.hum',\n",
       " 'kilroy',\n",
       " 'kilsmur.hum',\n",
       " 'kloo.txt',\n",
       " 'koans.txt',\n",
       " 'labels.txt',\n",
       " 'lampoon.jok',\n",
       " 'languag.jok',\n",
       " 'lansing.txt',\n",
       " 'law.sch',\n",
       " 'lawhunt.txt',\n",
       " 'laws.txt',\n",
       " 'lawskool.txt',\n",
       " 'lawsuniv.hum',\n",
       " 'lawyer.jok',\n",
       " 'lawyers.txt',\n",
       " 'lazarus.txt',\n",
       " 'la_times.hun',\n",
       " 'lbinter.hum',\n",
       " 'leech.txt',\n",
       " 'legal.hum',\n",
       " 'let.go',\n",
       " 'letgosh.txt',\n",
       " 'letter.txt',\n",
       " 'letterbx.txt',\n",
       " 'letter_f.sch',\n",
       " 'libraway.txt',\n",
       " 'liceprof.sty',\n",
       " 'lif&love.hum',\n",
       " 'lifeimag.hum',\n",
       " 'lifeinfo.hum',\n",
       " 'lifeonledge.txt',\n",
       " 'limerick.jok',\n",
       " 'lines.jok',\n",
       " 'lion.jok',\n",
       " 'lion.txt',\n",
       " 'lions.cat',\n",
       " 'lipkovits.txt',\n",
       " 'livnware.hum',\n",
       " 'llamas.txt',\n",
       " 'lll.hum',\n",
       " 'llong.hum',\n",
       " 'lobquad.hum',\n",
       " 'looser.hum',\n",
       " 'losers84.hum',\n",
       " 'losers86.hum',\n",
       " 'lost.txt',\n",
       " 'lotsa.jok',\n",
       " 'lozers',\n",
       " 'lozerzon.hum',\n",
       " 'lozeuser.hum',\n",
       " 'lp-assoc.txt',\n",
       " 'lucky.cha',\n",
       " 'ludeinfo.hum',\n",
       " 'ludeinfo.txt',\n",
       " 'luggage.hum',\n",
       " 'luvstory.txt',\n",
       " 'luzerzo2.hum',\n",
       " 'm0dzmen.hum',\n",
       " 'macsfarm.old',\n",
       " 'madhattr.jok',\n",
       " 'madscrib.hum',\n",
       " 'maecenas.hum',\n",
       " 'mailfrag.hum',\n",
       " 'makebeer.hum',\n",
       " 'making_y.wel',\n",
       " 'malechem.txt',\n",
       " 'manager.txt',\n",
       " 'manilla.hum',\n",
       " 'manners.txt',\n",
       " 'manspace.hum',\n",
       " 'margos.txt',\n",
       " 'marines.hum',\n",
       " 'marriage.hum',\n",
       " 'mash.hum',\n",
       " 'math.1',\n",
       " 'math.2',\n",
       " 'math.far',\n",
       " 'maxheadr',\n",
       " 'mcd.txt',\n",
       " 'mead.rcp',\n",
       " 'meat2.txt',\n",
       " 'meinkamp.hum',\n",
       " 'mel.txt',\n",
       " 'melodram.hum',\n",
       " 'memo.hum',\n",
       " 'memory.hum',\n",
       " 'men&wome.txt',\n",
       " 'mensroom.jok',\n",
       " 'merry.txt',\n",
       " 'miamadvi.hum',\n",
       " 'miami.hum',\n",
       " 'miamimth.txt',\n",
       " 'middle.age',\n",
       " 'mindvox',\n",
       " 'minn.txt',\n",
       " 'miranda.hum',\n",
       " 'misc.1',\n",
       " 'misery.hum',\n",
       " 'missdish',\n",
       " 'missheav.hum',\n",
       " 'mitch.txt',\n",
       " 'mlverb.hum',\n",
       " 'modemwld.txt',\n",
       " 'modest.hum',\n",
       " 'modstup',\n",
       " 'mog-history',\n",
       " 'montoys.txt',\n",
       " 'montpyth.hum',\n",
       " 'moonshin',\n",
       " 'moore.txt',\n",
       " 'moose.txt',\n",
       " 'moslem.txt',\n",
       " 'mothers.txt',\n",
       " 'motrbike.jok',\n",
       " 'mov_rail.txt',\n",
       " 'mowers.txt',\n",
       " 'mr.rogers',\n",
       " 'mrscienc.hum',\n",
       " 'mrsfield',\n",
       " 'msfields.txt',\n",
       " 'msorrow',\n",
       " 'mtm.hum',\n",
       " 'mtv.asc',\n",
       " 'mundane.v2',\n",
       " 'murph.jok',\n",
       " 'murphy.txt',\n",
       " 'murphys.txt',\n",
       " 'murphy_l.txt',\n",
       " 'mutate.hum',\n",
       " 'mydaywss.hum',\n",
       " 'myheart.hum',\n",
       " 'naivewiz.hum',\n",
       " 'namaste.txt',\n",
       " 'nameisreo.txt',\n",
       " 'namm',\n",
       " 'nasaglenn.txt',\n",
       " 'necropls.txt',\n",
       " 'netmask.txt',\n",
       " 'netnews.10',\n",
       " 'newcoke.txt',\n",
       " 'newconst.hum',\n",
       " 'newmex.hum',\n",
       " 'news.hum',\n",
       " 'nigel.1',\n",
       " 'nigel.10',\n",
       " 'nigel.2',\n",
       " 'nigel.3',\n",
       " 'nigel.4',\n",
       " 'nigel.5',\n",
       " 'nigel.6',\n",
       " 'nigel.7',\n",
       " 'nigel10.txt',\n",
       " 'nihgel_8.9',\n",
       " 'nintendo.jok',\n",
       " 'normal.boy',\n",
       " 'normalboy.txt',\n",
       " 'normquot.txt',\n",
       " 'nosuch_nasfic',\n",
       " 'novel.hum',\n",
       " 'nuke.hum',\n",
       " 'nukeplay.hum',\n",
       " 'nukewar.jok',\n",
       " 'nukewar.txt',\n",
       " 'nukwaste',\n",
       " 'number',\n",
       " 'number.killer',\n",
       " 'number_k.ill',\n",
       " 'nurds.hum',\n",
       " 'nysucks.hum',\n",
       " 'nzdrinks.txt',\n",
       " 'o-ttalk.hum',\n",
       " 'oakwood.txt',\n",
       " 'oam-001.txt',\n",
       " 'oam.nfo',\n",
       " 'oasis',\n",
       " 'oatbran.rec',\n",
       " 'oculis.rcp',\n",
       " 'odd_to.obs',\n",
       " 'odearakk.hum',\n",
       " 'office.txt',\n",
       " 'ohandre.hum',\n",
       " 'oilgluts.hum',\n",
       " 'old.txt',\n",
       " 'oldeng.hum',\n",
       " 'oldtime.sng',\n",
       " 'oldtime.txt',\n",
       " 'oliver.txt',\n",
       " 'oliver02.txt',\n",
       " 'onan.txt',\n",
       " 'one.par',\n",
       " 'onetoone.hum',\n",
       " 'onetotwo.hum',\n",
       " 'ookpik.hum',\n",
       " 'opinion.hum',\n",
       " 'oracle.jok',\n",
       " 'oranchic.pol',\n",
       " 'orgfrost.bev',\n",
       " 'ourfathr.txt',\n",
       " 'outawork.erl',\n",
       " 'outlimit.txt',\n",
       " 'oxymoron.jok',\n",
       " 'oxymoron.txt',\n",
       " 'ozarks.hum',\n",
       " 'p-law.hum',\n",
       " 'packard.txt',\n",
       " 'paddingurpapers.txt',\n",
       " 'parabl.hum',\n",
       " 'parades.hum',\n",
       " 'parsnip.txt',\n",
       " 'passage.hum',\n",
       " 'passenge.sim',\n",
       " 'pasta001.sal',\n",
       " 'pat.txt',\n",
       " 'pbcookie.des',\n",
       " 'peanuts.txt',\n",
       " 'peatchp.hum',\n",
       " 'pecker.txt',\n",
       " 'penisprt.txt',\n",
       " 'penndtch',\n",
       " 'pepper.txt',\n",
       " 'pepsideg.txt',\n",
       " 'petshop',\n",
       " 'phony.hum',\n",
       " 'phorse.hum',\n",
       " 'phunatdi.ana',\n",
       " 'phxbbs-m.txt',\n",
       " 'pickup.lin',\n",
       " 'pickup.txt',\n",
       " 'pipespec.txt',\n",
       " 'pizzawho.hum',\n",
       " 'planeget.hum',\n",
       " 'planetzero.txt',\n",
       " 'poets.hum',\n",
       " 'pol-corr.txt',\n",
       " 'polemom.txt',\n",
       " 'poli.tics',\n",
       " 'policpig.hum',\n",
       " 'poli_t.ics',\n",
       " 'poll2res.hum',\n",
       " 'polly.txt',\n",
       " 'polly_.new',\n",
       " 'poopie.txt',\n",
       " 'popconc.hum',\n",
       " 'popmach',\n",
       " 'popmusi.hum',\n",
       " 'post.nuc',\n",
       " 'pot.txt',\n",
       " 'potty.txt',\n",
       " 'pournell.spo',\n",
       " 'ppbeer.txt',\n",
       " 'prac1.jok',\n",
       " 'prac2.jok',\n",
       " 'prac3.jok',\n",
       " 'prac4.jok',\n",
       " 'pracjoke.txt',\n",
       " 'practica.txt',\n",
       " 'prawblim.hum',\n",
       " 'prayer.hum',\n",
       " 'primes.jok',\n",
       " 'princess.brd',\n",
       " 'pro-fact.hum',\n",
       " 'problem.txt',\n",
       " 'progrs.gph',\n",
       " 'proof.met',\n",
       " 'prooftec.txt',\n",
       " 'proposal.jok',\n",
       " 'proudlyserve.txt',\n",
       " 'prover.wisom',\n",
       " 'prover_w.iso',\n",
       " 'psalm.reagan',\n",
       " 'psalm23.txt',\n",
       " 'psalm_nixon',\n",
       " 'psalm_re.aga',\n",
       " 'psilaine.hum',\n",
       " 'psycho.txt',\n",
       " 'psych_pr.quo',\n",
       " 'pukeprom.jok',\n",
       " 'pun.txt',\n",
       " 'pure.mat',\n",
       " 'puzzle.spo',\n",
       " 'puzzles.jok',\n",
       " 'python_s.ong',\n",
       " 'q.pun',\n",
       " 'qttofu.vgn',\n",
       " 'quack26.txt',\n",
       " 'quantity.001',\n",
       " 'quantum.jok',\n",
       " 'quantum.phy',\n",
       " 'quest.hum',\n",
       " 'quick.jok',\n",
       " 'quotes.bug',\n",
       " 'quotes.jok',\n",
       " 'quotes.txt',\n",
       " 'quux_p.oem',\n",
       " 'rabbit.txt',\n",
       " 'racist.net',\n",
       " 'radexposed.txt',\n",
       " 'radiolaf.hum',\n",
       " 'rapmastr.hum',\n",
       " 'ratings.hum',\n",
       " 'ratspit.hum',\n",
       " 'raven.hum',\n",
       " 'readme.bat',\n",
       " 'reagan.hum',\n",
       " 'realest.txt',\n",
       " 'reasons.txt',\n",
       " 'rec.por',\n",
       " 'recepies.fun',\n",
       " 'recip1.txt',\n",
       " 'recipe.001',\n",
       " 'recipe.002',\n",
       " 'recipe.003',\n",
       " 'recipe.004',\n",
       " 'recipe.005',\n",
       " 'recipe.006',\n",
       " 'recipe.007',\n",
       " 'recipe.008',\n",
       " 'recipe.009',\n",
       " 'recipe.010',\n",
       " 'recipe.011',\n",
       " 'recipe.012',\n",
       " 'reconcil.hum',\n",
       " 'record_.gap',\n",
       " 'red-neck.jks',\n",
       " 'reddwarf.sng',\n",
       " 'reddye.hum',\n",
       " 'rednecks.txt',\n",
       " 'reeves.txt',\n",
       " 'relative.ada',\n",
       " 'religion.txt',\n",
       " 'renored.txt',\n",
       " 'renorthr.txt',\n",
       " 'rent-a_cat',\n",
       " 'rentals.hum',\n",
       " 'repair.hum',\n",
       " 'report.hum',\n",
       " 'research.hum',\n",
       " 'residncy.jok',\n",
       " 'resolutn.txt',\n",
       " 'resrch_p.hra',\n",
       " 'resrch_phrase',\n",
       " 'revolt.dj',\n",
       " 'richbred.txt',\n",
       " 'rinaldo.jok',\n",
       " 'rinaldos.law',\n",
       " 'rinaldos.txt',\n",
       " 'ripoffpc.hum',\n",
       " 'rns_bcl.txt',\n",
       " 'rns_bwl.txt',\n",
       " 'rns_ency.txt',\n",
       " 'roach.asc',\n",
       " 'roadpizz.txt',\n",
       " 'robot.tes',\n",
       " 'rocking.hum',\n",
       " 'rockmus.hum',\n",
       " 'sanshop.txt',\n",
       " 'saveface.hum',\n",
       " 'sawyer.txt',\n",
       " 'scam.txt',\n",
       " 'scratchy.txt',\n",
       " 'seafood.txt',\n",
       " 'seeds42.txt',\n",
       " 'sf-zine.pub',\n",
       " 'sfmovie.txt',\n",
       " 'shameonu.hum',\n",
       " 'shooters.txt',\n",
       " 'shorties.jok',\n",
       " 'shrink.news',\n",
       " 'shuimai.txt',\n",
       " 'shuttleb.hum',\n",
       " 'signatur.jok',\n",
       " 'sigs.txt',\n",
       " 'silverclaws.txt',\n",
       " 'simp.txt',\n",
       " 'sinksub.txt',\n",
       " 'skincat',\n",
       " 'skippy.hum',\n",
       " 'skippy.txt',\n",
       " 'slogans.txt',\n",
       " 'smackjok.hum',\n",
       " 'smartass.txt',\n",
       " 'smiley.txt',\n",
       " 'smokers.txt',\n",
       " 'smurf-03.txt',\n",
       " 'smurfkil.hum',\n",
       " 'smurfs.cc',\n",
       " 'smurf_co.txt',\n",
       " 'snapple.rum',\n",
       " 'snipe.txt',\n",
       " 'soccer.txt',\n",
       " 'socecon.hum',\n",
       " 'social.hum',\n",
       " 'socks.drx',\n",
       " 'solders.hum',\n",
       " 'soleleer.hum',\n",
       " 'solviets.hum',\n",
       " 'some_hu.mor',\n",
       " 'soporifi.abs',\n",
       " 'sorority.gir',\n",
       " 'spacever.hum',\n",
       " 'speling.msk',\n",
       " 'spelin_r.ifo',\n",
       " 'spider.hum',\n",
       " 'spoonlis.txt',\n",
       " 'spydust.hum',\n",
       " 'squids.gph',\n",
       " 'staff.txt',\n",
       " 'stagline.txt',\n",
       " 'standard.hum',\n",
       " 'startrek.txt',\n",
       " 'stereo.txt',\n",
       " 'steroid.txt',\n",
       " 'stone.hum',\n",
       " 'strattma.txt',\n",
       " 'stressman.txt',\n",
       " 'strine.txt',\n",
       " 'strsdiet.txt',\n",
       " 'studentb.txt',\n",
       " 'stuf10.txt',\n",
       " 'stuf11.txt',\n",
       " 'st_silic.txt',\n",
       " 'subb_lis.txt',\n",
       " 'subrdead.hum',\n",
       " 'suicide2.txt',\n",
       " 'sungenu.hum',\n",
       " 'supermar.rul',\n",
       " 'swearfrn.hum',\n",
       " 'sw_err.txt',\n",
       " 'symbol.hum',\n",
       " 'sysadmin.txt',\n",
       " 'sysman.txt',\n",
       " 't-10.hum',\n",
       " 't-shirt.hum',\n",
       " 'takenote.jok',\n",
       " 'talebeat.hum',\n",
       " 'talkbizr.txt',\n",
       " 'taping.hum',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2e58a",
   "metadata": {},
   "source": [
    "2. To calculate this make set of the document token and query token and perform intersection and union between the query and each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9e2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Jaccard Coefficient\n",
    "def calculate_jaccard(query, document):\n",
    "    query = preProcess(query)\n",
    "    doc = preProcess(document)\n",
    "    intersections = len(set(query)&set(doc))    # Finding the intersections between query and document\n",
    "    unions = len(set(query)|set(doc))           # Finding the unions between query and document\n",
    "    jaccard = intersections/unions              # Calculate Jaccard coefficient\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdb0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving Top 5 Documents using Jaccard Coefficient\n",
    "\n",
    "def jaccard_retrieval_docs(query,files):\n",
    "    docs_Dict={}                       \n",
    "    jaccard_coefficients_list=[]           # List of jaccard coefficient for each doc\n",
    "    i=0\n",
    "    for file in files:\n",
    "        docs_Dict[i]=file\n",
    "        try:\n",
    "            f = open(\"./Files/\"+file, \"r\")\n",
    "            data = f.read()\n",
    "        except:\n",
    "            f = open(\"./Files/\"+file, \"rb\")\n",
    "            data = f.read().decode('utf-8', 'backslashreplace')\n",
    "        jaccard_coefficients_list.append(calculate_jaccard(query,data))      \n",
    "        i+=1\n",
    "    jaccard_coefficients_list = np.array(jaccard_coefficients_list)\n",
    "    top_5_docs = jaccard_coefficients_list.argsort()[-5:][::-1]       # Finding the top 5 documents from the end.\n",
    "    for i in range(len(top_5_docs)):\n",
    "        print(\"Jaccard Coefficient: \", jaccard_coefficients_list[top_5_docs[i]], \", Document: \" ,docs_Dict[top_5_docs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c0883b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryInput():\n",
    "    q = input(\"Enter query: \")\n",
    "    q = preProcess(q)\n",
    "    print(q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16451c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sherlock\n"
     ]
    }
   ],
   "source": [
    "query=input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7ad69",
   "metadata": {},
   "source": [
    "3. Report the top 5 relevant documents based on the value of the Jaccard coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9acd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Coefficient:  0.005780346820809248 , Document:  livnware.hum\n",
      "Jaccard Coefficient:  0.0016863406408094434 , Document:  subb_lis.txt\n",
      "Jaccard Coefficient:  0.0016339869281045752 , Document:  mov_rail.txt\n",
      "Jaccard Coefficient:  0.0005605381165919282 , Document:  crzycred.lst\n",
      "Jaccard Coefficient:  0.0005125576627370579 , Document:  insults1.txt\n"
     ]
    }
   ],
   "source": [
    "jaccard_retrieval_docs(query,files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642af763",
   "metadata": {},
   "source": [
    "## TF-IDF Matrix \n",
    "\n",
    "The goal is to generate a TF-IDF matrix for each word in the vocab and obtain a TF-IDF score for a\n",
    "given query. TF-IDF has two parts Term Frequency and Inverse Document Frequency.\n",
    "<br><br>\n",
    "\n",
    "1. Computing Term Frequency involves calculating the raw count of the word in each document and\n",
    "stored as a nested dictionary for each document.\n",
    "2. To calculate the document frequency of each word, find the postings list of each word and subsequently\n",
    "find the no. of documents in each posting list of each word.\n",
    "3. The IDF value of each word is calculated using the formula as mention below:\n",
    "Using smoothing:-\n",
    "IDF(word)=log(total no. of documents/document frequency(word)+1)\n",
    "4. The Term Frequency is calculated using 5 different variants:\n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45faa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "tokens= []\n",
    "ID_File_Mapping = []\n",
    " \n",
    "# Reading each file and creating tokens for each file\n",
    "for file in os.listdir('./Files'):\n",
    "    try:\n",
    "        f = open(\"./Files/\"+file, \"r\") \n",
    "        text = \" \".join(f.read().split()) \n",
    "    except:\n",
    "        f = open(\"./Files/\"+file, \"rb\")      \n",
    "        text = f.read().decode('utf-8', 'backslashreplace')\n",
    "    f.close()\n",
    "\n",
    "    name = os.path.basename(file)\n",
    "    ID_File_Mapping.append([name])      # ID- Name Mapping of the file\n",
    "    \n",
    "#     print(text)\n",
    "    tokens.append(preProcess(text))  \n",
    "    \n",
    "ID_File_Mapping = pd.DataFrame(ID_File_Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f951f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1134"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dd373",
   "metadata": {},
   "source": [
    "### Calculating Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db4bedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Frequency - no. of documents in each posting list of each word.\n",
    "doc_frequency = {}   \n",
    "for t in tokens:\n",
    "    words = list(set(t))\n",
    "    for w in words:\n",
    "        if doc_frequency.get(w) == None:\n",
    "            doc_frequency[w] = 1   # If new word found, count=1\n",
    "        else:\n",
    "            count = doc_frequency.get(w)\n",
    "            count += 1              #else increase the frequency\n",
    "            doc_frequency[w] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3cf9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84028\n",
      "Total Number of documents are 1134\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_frequency))\n",
    "total_docs = len(files)\n",
    "print('Total Number of documents are',total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b164ff",
   "metadata": {},
   "source": [
    "#### The IDF value of each word is calculated using the formula as mention below: Using smoothing:- IDF(word)=log(total no. of documents/document frequency(word)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27fc958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency IDF(word)=log(total no. of documents/document frequency(word)+1)\n",
    "IDF = {}\n",
    "index = doc_frequency.keys()\n",
    "for i in index:\n",
    "    freq = doc_frequency[i]   #document frequency of each word in document frquency is picked\n",
    "    inv_doc_freq = np.log(total_docs/freq +1)\n",
    "    IDF[i] = inv_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f97658df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function block for Calculating tf-idf value---tf*idf as tf=idf will be calculated based on all the 5 variants\n",
    "def tf_idf_score(tf):\n",
    "    tf_idf = []  #creating list of list of tf-idf\n",
    "    for score in tf:\n",
    "        tf_idf_dict = {}\n",
    "        indexes = score.keys()\n",
    "        for w in indexes:\n",
    "            tf_idf_val = score[w] * IDF[w]                #Multiplying tfidf=tf*idf\n",
    "            tf_idf_dict[w] = tf_idf_val\n",
    "        tf_idf.append(tf_idf_dict)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4090e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching rank document based on tfidf\n",
    "\n",
    "def tfidf_rank_doc(tfidf_matrix, query):\n",
    "    \n",
    "    query_res = [0.0] * len(total_indexes)\n",
    "    res = [i for i, q in enumerate(total_indexes) if q in set(query)]\n",
    "    for i in range(len(total_indexes)):\n",
    "        if i in res: \n",
    "            query_res[i]=1.0     \n",
    "        else: \n",
    "            query_res[i]=0.0\n",
    "        \n",
    "    tf_docs = {}\n",
    "    for i in range(total_docs):\n",
    "        tf_docs[i] = 0.0\n",
    "    for r in res:\n",
    "        count = -1\n",
    "        for v in tfidf_matrix:\n",
    "            count += 1\n",
    "            tf_docs[count] += v[r]\n",
    "    tf_docs = dict(sorted(tf_docs.items(),key=operator.itemgetter(1),reverse=True))   \n",
    "\n",
    "    return tf_docs, query_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58d2876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrive top 5 documents based on variant matrix\n",
    "def Retrieve_Top_5_Docs(tfidf_matrix ,query, var):\n",
    "    tf_docs , query_res= tfidf_rank_doc(tfidf_matrix, query)\n",
    "    count = -1\n",
    "    temp = 0\n",
    "    final = []\n",
    "    for item in tf_docs.keys():\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            print('Top 5 Documents based on ',var ,' tf-idf are ',final)\n",
    "            for i in final:\n",
    "                print(\"TF_IDF Score: \", tf_docs[i], \"Document: \",ID_File_Mapping.iloc[i][0])\n",
    "            temp = 1\n",
    "            break\n",
    "        final.append(item)\n",
    "    if temp == 0:\n",
    "        print('Top 5 Documents based on ',var ,' tf-idf are :',final)    \n",
    "        for i in final:\n",
    "            print(\"TF_IDF Score: \", tf_docs[i], \"Document: \",ID_File_Mapping.iloc[i][0])\n",
    "            \n",
    "    return query_res         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c22ccd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84028"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_indexes = list(doc_frequency.keys())\n",
    "len(total_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e2c14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_word_count = []   #list for storing word count for each doc\n",
    "for item in tokens:\n",
    "    raw_word_doc ={}      \n",
    "    for w in item:\n",
    "        if raw_word_doc.get(w) == None:  \n",
    "            raw_word_doc[w] = 1       # If new word found, count=1\n",
    "        else:\n",
    "            count = raw_word_doc.get(w)\n",
    "            count += 1               #else increase the count\n",
    "            raw_word_doc[w] = count\n",
    "    raw_word_count.append(raw_word_doc)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60cf8c",
   "metadata": {},
   "source": [
    "## 1. Binary Variant - 0,1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12b18ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency is calculated using 5 different variants:\n",
    "\n",
    "binary_frequency = []   \n",
    "for word in raw_word_count:\n",
    "    binary_dict = {}        \n",
    "    indexes = word.keys()\n",
    "    for i in indexes:\n",
    "        if(word.get(i) > 0):\n",
    "            bool_freq = 1\n",
    "        else: \n",
    "            bool_freq = 0             \n",
    "        binary_dict[i] = bool_freq\n",
    "        \n",
    "    binary_frequency.append(binary_dict)\n",
    "\n",
    "binary_tf_idf_score = tf_idf_score(binary_frequency)\n",
    "\n",
    "#tf-idf matrix using binary\n",
    "binary_matrix = []   \n",
    "for score in binary_tf_idf_score:\n",
    "    bin_doc = []\n",
    "    for s in total_indexes:\n",
    "        if s in score :            \n",
    "            bin_doc.append(score[s])   #if word present then store tfidf\n",
    "        else:\n",
    "            bin_doc.append(0.0)    #else 0\n",
    "    binary_matrix += [bin_doc]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1778dc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: A simple device\n",
      "['simple', 'device']\n",
      "Top 5 Documents based on  Binary  tf-idf are  [9, 29, 35, 140, 173]\n",
      "TF_IDF Score:  4.884552500454063 Document:  acronyms.txt\n",
      "TF_IDF Score:  4.884552500454063 Document:  all_grai\n",
      "TF_IDF Score:  4.884552500454063 Document:  anime.cli\n",
      "TF_IDF Score:  4.884552500454063 Document:  bored.txt\n",
      "TF_IDF Score:  4.884552500454063 Document:  bw.txt\n"
     ]
    }
   ],
   "source": [
    "q = queryInput()  \n",
    "binary_query = Retrieve_Top_5_Docs(binary_matrix,q,'Binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed01d1",
   "metadata": {},
   "source": [
    "### Pros\n",
    "1. Binary word count is simple and fast to create. <br>\n",
    "2. Process large documents collections quickly. Takes only O(n) time.\n",
    "\n",
    "\n",
    "### Cons\n",
    "1. Treat every word equally. And due to this it does not take into account that some characters occuring more frequently might provide some info but this is not captured in 0-1\n",
    "2. With the increase in the vocab size, length of vectors increase too which makes this variant slow.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7c089",
   "metadata": {},
   "source": [
    "## 2. Raw Word Count - f(t,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f6f677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tf_idf_score = tf_idf_score(raw_word_count)\n",
    "\n",
    "#tf-idf score using raw count\n",
    "raw_matrix = []   \n",
    "for word in raw_tf_idf_score:\n",
    "    raw_docs = []\n",
    "    for w in total_indexes:\n",
    "        if w in word :\n",
    "            raw_docs.append(word[w])   #if word present then store tfidf\n",
    "        else:\n",
    "            raw_docs.append(0.0)     #else 0\n",
    "    raw_matrix += [raw_docs]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c120b570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: A simple device\n",
      "['simple', 'device']\n",
      "Top 5 Documents based on  Raw Count  tf-idf are  [9, 1069, 501, 113, 500]\n",
      "TF_IDF Score:  39.94560674024448 Document:  acronyms.txt\n",
      "TF_IDF Score:  34.45897006036514 Document:  variety3.asc\n",
      "TF_IDF Score:  33.589783323753174 Document:  humor9.txt\n",
      "TF_IDF Score:  32.919820079276015 Document:  bimg.prn\n",
      "TF_IDF Score:  32.919820079276015 Document:  humatran.jok\n"
     ]
    }
   ],
   "source": [
    "q = queryInput()  \n",
    "raw_query = Retrieve_Top_5_Docs(raw_matrix,q,'Raw Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcef161",
   "metadata": {},
   "source": [
    "### Pros\n",
    "1. Easy to compute\n",
    "2. Used to extract the most descriptive terms in a document\n",
    "\n",
    "### Cons\n",
    "1. It is slow for large dictionaries as it directly computes word count space  for document similarity.\n",
    "2. it does not capture semantics, position in text, co-occurrences in different documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b1954",
   "metadata": {},
   "source": [
    "##  3. Term Frequency - f(t,d)/Pf(tâ€˜, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "618aafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = []     #list of log normalization term frequency\n",
    "for word in raw_word_count:\n",
    "    term_dict = {}        \n",
    "    indexes = word.keys()\n",
    "    term_sum = sum(word.values())     #counting no. of tokens in each document\n",
    "    for i in indexes:\n",
    "        term_dict[i] = (word.get(i) / term_sum)\n",
    "    term_frequency.append(term_dict)\n",
    "    \n",
    "term_tf_idf_score = tf_idf_score(term_frequency)\n",
    "\n",
    "##tf-idf score using term frequency\n",
    "term_matrix = []  \n",
    "for score in term_tf_idf_score:\n",
    "    term_docs = []\n",
    "    for i in total_indexes:\n",
    "        if i in score :\n",
    "            term_docs.append(score[i])    #if word present then store tfidf\n",
    "        else:\n",
    "            term_docs.append(0.0)    #else 0\n",
    "    term_matrix += [term_docs]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0094b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: A simple device\n",
      "['simple', 'device']\n",
      "Top 5 Documents based on  Term Frequency  tf-idf are  [500, 499, 58, 798, 113]\n",
      "TF_IDF Score:  0.04308876973727227 Document:  humatran.jok\n",
      "TF_IDF Score:  0.04191180797130048 Document:  humatra.txt\n",
      "TF_IDF Score:  0.03305202819204419 Document:  atombomb.hum\n",
      "TF_IDF Score:  0.026112611713590195 Document:  poli.tics\n",
      "TF_IDF Score:  0.02600301744018643 Document:  bimg.prn\n"
     ]
    }
   ],
   "source": [
    "q = queryInput()  \n",
    "\n",
    "term_query = Retrieve_Top_5_Docs(term_matrix,q,'Term Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4fee94",
   "metadata": {},
   "source": [
    "### Pros\n",
    "1. When number of times occurences of word is more important than the word. We use Term frquency.\n",
    "2. Helps in extracting the most descriptive word in a document.\n",
    "\n",
    "### Cons\n",
    "1. It cannot extracts semantic similiarities.\n",
    "2. It can suffer from memory inefficiency for large document collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7926be1",
   "metadata": {},
   "source": [
    "## 4. Log Normalisation = log(1 +f(t,d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2afe5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_frequency = []      \n",
    "for word in raw_word_count:\n",
    "    log_dict = {}         #dictionary for log normalization term frequency\n",
    "    indexes = word.keys()\n",
    "    for i in indexes:    \n",
    "        freq = word.get(i)\n",
    "        log_dict[i] = np.log(1 + freq)     # log(1 +f(t,d)))\n",
    "    log_frequency.append(log_dict)\n",
    "    \n",
    "    \n",
    "log_tf_idf_score = tf_idf_score(log_frequency)\n",
    "\n",
    "log_matrix = []  \n",
    "for score in log_tf_idf_score:\n",
    "    log_docs = []\n",
    "    for s in total_indexes:\n",
    "        if s in score :            \n",
    "            log_docs.append(score[s])    #if word present then store tfidf\n",
    "        else:\n",
    "            log_docs.append(0.0)    #else 0\n",
    "    log_matrix += [log_docs]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18a7f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: A simple device\n",
      "['simple', 'device']\n",
      "Top 5 Documents based on  Logarithmic  tf-idf are  [501, 35, 9, 1069, 451]\n",
      "TF_IDF Score:  10.043023323092292 Document:  humor9.txt\n",
      "TF_IDF Score:  9.86426306290889 Document:  anime.cli\n",
      "TF_IDF Score:  9.592160534142803 Document:  acronyms.txt\n",
      "TF_IDF Score:  9.169276147058518 Document:  variety3.asc\n",
      "TF_IDF Score:  9.1199728654888 Document:  hackmorality.txt\n"
     ]
    }
   ],
   "source": [
    "q = queryInput()  \n",
    "log_query = Retrieve_Top_5_Docs(log_matrix,q,'Logarithmic')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5262dac",
   "metadata": {},
   "source": [
    "### Pros\n",
    "1. When the occurences of the term is required as an important feature then log weighting is used. For example a word occuring 20 times in a document is more relevant but not 20 times more relevant. So here we will use log normalization to calculate term frequency.\n",
    "2. It prevent bias in term frequency from terms in shorter or longer documents\n",
    "\n",
    "### Cons\n",
    "1. It also does not help in capturing semantic structures ovee other documents \n",
    "2. Can gives more importance to less important but more occurrence word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ba914",
   "metadata": {},
   "source": [
    "## 5. Double Normalisation Frequency = 0.5 + ((0.5*tf)/max(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c4852c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_frequency = []  #list for storing double normalization term frequency for each doc\n",
    "\n",
    "\n",
    "for word in raw_word_count:\n",
    "    double_dict = {}             #dictionary for storing double normalization term frequency\n",
    "    indexes = word.keys()\n",
    "    for i in indexes:\n",
    "        double_dict[i] = 0.5 + (0.5*(word.get(i) / max(word.values())))    #Double Normalisation Frequency = 0.5 + ((0.5*tf)/max(tf))\n",
    "    double_frequency.append(double_dict)\n",
    "    \n",
    "double_tf_idf_score = tf_idf_score(double_frequency)\n",
    "\n",
    "#tf-idf score using double\n",
    "double_matrix = []    \n",
    "for score in double_tf_idf_score:\n",
    "    double_docs = []\n",
    "    for s in total_indexes:\n",
    "        if s in score :\n",
    "            double_docs.append(score[s])    #if word present then store tfidf\n",
    "        else:\n",
    "            double_docs.append(0.0)    #else 0\n",
    "    double_matrix += [double_docs]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b3e55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: A simple device\n",
      "['simple', 'device']\n",
      "Top 5 Documents based on  Double  tf-idf are  [35, 751, 1069, 9, 423]\n",
      "TF_IDF Score:  2.7189941953294836 Document:  anime.cli\n",
      "TF_IDF Score:  2.6835136402660544 Document:  onan.txt\n",
      "TF_IDF Score:  2.6473891672530145 Document:  variety3.asc\n",
      "TF_IDF Score:  2.5935853666673516 Document:  acronyms.txt\n",
      "TF_IDF Score:  2.5788857035632096 Document:  gd_ol.txt\n"
     ]
    }
   ],
   "source": [
    "q = queryInput()  \n",
    "\n",
    "double_query = Retrieve_Top_5_Docs(double_matrix,q,'Double') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc318eb",
   "metadata": {},
   "source": [
    "### Pros\n",
    "\n",
    "1. Useful when one document is very large and word occurences is just due to large voab and not its relevancy. It normalize the tf weights of all terms occurring in a document by the maximum tf in that document.\n",
    "2. It prevents a bias towards longer document by normalizing the raw frequency of the term into a comparable scale.\n",
    "\n",
    "### Cons\n",
    "\n",
    "1. The method is unstable and hard to tune because a change in the stop word list can dramatically alter term weightings and ranking too.<br>\n",
    "2. Outlier term which is occuring large number of times in a document may become representative of the content of that document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8679791",
   "metadata": {},
   "source": [
    "## Compute the TF-IDF score for the query using the TF-IDF matrix. \n",
    " \n",
    "### Report the top 5 relevant documents based on the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab676f",
   "metadata": {},
   "source": [
    "1. Binary TFIDF\n",
    "\n",
    "Top 5 Documents based on  Binary  tf-idf are  [9, 29, 35, 140, 173]\n",
    "<br><br>\n",
    "TF_IDF Score:  4.884552500454063 Document:  acronyms.txt\n",
    "<br>\n",
    "TF_IDF Score:  4.884552500454063 Document:  all_grai\n",
    "<br>\n",
    "TF_IDF Score:  4.884552500454063 Document:  anime.cli\n",
    "<br>\n",
    "TF_IDF Score:  4.884552500454063 Document:  bored.txt\n",
    "<br>\n",
    "TF_IDF Score:  4.884552500454063 Document:  bw.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdd779",
   "metadata": {},
   "source": [
    "2. Raw Word  Count TF=iDF\n",
    "\n",
    "Top 5 Documents based on  Raw Count  tf-idf are  [9, 1069, 501, 113, 500]<br><br>\n",
    "TF_IDF Score:  39.94560674024448 Document:  acronyms.txt<br>\n",
    "TF_IDF Score:  34.45897006036514 Document:  variety3.asc<br>\n",
    "TF_IDF Score:  33.589783323753174 Document:  humor9.txt<br>\n",
    "TF_IDF Score:  32.919820079276015 Document:  bimg.prn<br>\n",
    "TF_IDF Score:  32.919820079276015 Document:  humatran.jok<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e076513",
   "metadata": {},
   "source": [
    "3. Term Frequency TF-IDF\n",
    "\n",
    "Top 5 Documents based on  Term Frequency  tf-idf are  [500, 499, 58, 798, 113]<br><br>\n",
    "TF_IDF Score:  0.04308876973727227 Document:  humatran.jok<br>\n",
    "TF_IDF Score:  0.04191180797130048 Document:  humatra.txt<br>\n",
    "TF_IDF Score:  0.03305202819204419 Document:  atombomb.hum<br>\n",
    "TF_IDF Score:  0.026112611713590195 Document:  poli.tics<br>\n",
    "TF_IDF Score:  0.02600301744018643 Document:  bimg.prn<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82dd6ac",
   "metadata": {},
   "source": [
    "4. Lograithmicc TF-IDF\n",
    "\n",
    "Top 5 Documents based on  Logarithmic  tf-idf are  [501, 35, 9, 1069, 451]<br><br>\n",
    "TF_IDF Score:  10.043023323092292 Document:  humor9.txt<br>\n",
    "TF_IDF Score:  9.86426306290889 Document:  anime.cli<br>\n",
    "TF_IDF Score:  9.592160534142803 Document:  acronyms.txt<br>\n",
    "TF_IDF Score:  9.169276147058518 Document:  variety3.asc<br>\n",
    "TF_IDF Score:  9.1199728654888 Document:  hackmorality.txt<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e3ec7",
   "metadata": {},
   "source": [
    "5. Double TF-IDF\n",
    "\n",
    "Top 5 Documents based on  Double  tf-idf are  [35, 751, 1069, 9, 423]<br><br>\n",
    "TF_IDF Score:  2.7189941953294836 Document:  anime.cli<br>\n",
    "TF_IDF Score:  2.6835136402660544 Document:  onan.txt<br>\n",
    "TF_IDF Score:  2.6473891672530145 Document:  variety3.asc<br>\n",
    "TF_IDF Score:  2.5935853666673516 Document:  acronyms.txt<br>\n",
    "TF_IDF Score:  2.5788857035632096 Document:  gd_ol.txt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c6cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
